{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of data usage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Following code adapted from NeiA-PyTorch (https://github.com/amorehead/NeiA-PyTorch):\n",
    "# -------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "from pytorch_lightning.plugins import DDPPlugin\n",
    "\n",
    "from project.datasets.DB5.db5_dgl_data_module import DB5DGLDataModule\n",
    "from project.utils.modules import LitNeiA\n",
    "from project.utils.training_utils import collect_args, process_args, construct_pl_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    # -----------\n",
    "    # Data\n",
    "    # -----------\n",
    "    # Load Docking Benchmark 5 (DB5) data module\n",
    "    db5_data_module = DB5DGLDataModule(data_dir=args.db5_data_dir,\n",
    "                                       batch_size=args.batch_size,\n",
    "                                       num_dataloader_workers=args.num_workers,\n",
    "                                       knn=args.knn,\n",
    "                                       self_loops=args.self_loops,\n",
    "                                       percent_to_use=args.db5_percent_to_use,\n",
    "                                       process_complexes=args.process_complexes,\n",
    "                                       input_indep=args.input_indep)\n",
    "    db5_data_module.setup()\n",
    "\n",
    "    # ------------\n",
    "    # Model\n",
    "    # ------------\n",
    "    # Assemble a dictionary of model arguments\n",
    "    dict_args = vars(args)\n",
    "    use_wandb_logger = args.logger_name.lower() == 'wandb'  # Determine whether the user requested to use WandB\n",
    "\n",
    "    # Pick model and supply it with a dictionary of arguments\n",
    "    if args.model_name.lower() == 'neiwa':  # Neighborhood Weighted Average (NeiWA)\n",
    "        model = LitNeiA(num_node_input_feats=db5_data_module.db5_test.num_node_features,\n",
    "                        num_edge_input_feats=db5_data_module.db5_test.num_edge_features,\n",
    "                        gnn_activ_fn=nn.Tanh(),\n",
    "                        interact_activ_fn=nn.ReLU(),\n",
    "                        num_classes=db5_data_module.db5_test.num_classes,\n",
    "                        weighted_avg=True,  # Use the neighborhood weighted average variant of NeiA\n",
    "                        num_gnn_layers=dict_args['num_gnn_layers'],\n",
    "                        num_interact_layers=dict_args['num_interact_layers'],\n",
    "                        num_interact_hidden_channels=dict_args['num_interact_hidden_channels'],\n",
    "                        num_epochs=dict_args['num_epochs'],\n",
    "                        pn_ratio=dict_args['pn_ratio'],\n",
    "                        knn=dict_args['knn'],\n",
    "                        dropout_rate=dict_args['dropout_rate'],\n",
    "                        metric_to_track=dict_args['metric_to_track'],\n",
    "                        weight_decay=dict_args['weight_decay'],\n",
    "                        batch_size=dict_args['batch_size'],\n",
    "                        lr=dict_args['lr'],\n",
    "                        multi_gpu_backend=dict_args[\"accelerator\"])\n",
    "        args.experiment_name = f'LitNeiWA-b{args.batch_size}-gl{args.num_gnn_layers}' \\\n",
    "                               f'-n{db5_data_module.db5_test.num_node_features}' \\\n",
    "                               f'-e{db5_data_module.db5_test.num_edge_features}' \\\n",
    "                               f'-il{args.num_interact_layers}-i{args.num_interact_hidden_channels}' \\\n",
    "            if not args.experiment_name \\\n",
    "            else args.experiment_name\n",
    "        template_ckpt_filename = 'LitNeiWA-{epoch:02d}-{val_ce:.2f}'\n",
    "\n",
    "    else:  # Default Model - Neighborhood Average (NeiA)\n",
    "        model = LitNeiA(num_node_input_feats=db5_data_module.db5_test.num_node_features,\n",
    "                        num_edge_input_feats=db5_data_module.db5_test.num_edge_features,\n",
    "                        gnn_activ_fn=nn.Tanh(),\n",
    "                        interact_activ_fn=nn.ReLU(),\n",
    "                        num_classes=db5_data_module.db5_test.num_classes,\n",
    "                        weighted_avg=False,\n",
    "                        num_gnn_layers=dict_args['num_gnn_layers'],\n",
    "                        num_interact_layers=dict_args['num_interact_layers'],\n",
    "                        num_interact_hidden_channels=dict_args['num_interact_hidden_channels'],\n",
    "                        num_epochs=dict_args['num_epochs'],\n",
    "                        pn_ratio=dict_args['pn_ratio'],\n",
    "                        knn=dict_args['knn'],\n",
    "                        dropout_rate=dict_args['dropout_rate'],\n",
    "                        metric_to_track=dict_args['metric_to_track'],\n",
    "                        weight_decay=dict_args['weight_decay'],\n",
    "                        batch_size=dict_args['batch_size'],\n",
    "                        lr=dict_args['lr'],\n",
    "                        multi_gpu_backend=dict_args[\"accelerator\"])\n",
    "        args.experiment_name = f'LitNeiA-b{args.batch_size}-gl{args.num_gnn_layers}' \\\n",
    "                               f'-n{db5_data_module.db5_test.num_node_features}' \\\n",
    "                               f'-e{db5_data_module.db5_test.num_edge_features}' \\\n",
    "                               f'-il{args.num_interact_layers}-i{args.num_interact_hidden_channels}' \\\n",
    "            if not args.experiment_name \\\n",
    "            else args.experiment_name\n",
    "        template_ckpt_filename = 'LitNeiA-{epoch:02d}-{val_ce:.2f}'\n",
    "\n",
    "    # ------------\n",
    "    # Checkpoint\n",
    "    # ------------\n",
    "    ckpt_path = os.path.join(args.ckpt_dir, args.ckpt_name)\n",
    "    ckpt_path_exists = os.path.exists(ckpt_path)\n",
    "    ckpt_provided = args.ckpt_name != '' and ckpt_path_exists\n",
    "    model = model.load_from_checkpoint(ckpt_path,\n",
    "                                       use_wandb_logger=use_wandb_logger,\n",
    "                                       batch_size=args.batch_size,\n",
    "                                       lr=args.lr,\n",
    "                                       weight_decay=args.weight_decay,\n",
    "                                       dropout_rate=args.dropout_rate) if ckpt_provided else model\n",
    "\n",
    "    # ------------\n",
    "    # Trainer\n",
    "    # ------------\n",
    "    trainer = pl.Trainer.from_argparse_args(args)\n",
    "\n",
    "    # -------------\n",
    "    # Learning Rate\n",
    "    # -------------\n",
    "    if args.find_lr:\n",
    "        lr_finder = trainer.tuner.lr_find(model, datamodule=db5_data_module)  # Run learning rate finder\n",
    "        fig = lr_finder.plot(suggest=True)  # Plot learning rates\n",
    "        fig.savefig('optimal_lr.pdf')\n",
    "        fig.show()\n",
    "        model.hparams.lr = lr_finder.suggestion()  # Save optimal learning rate\n",
    "        print(f'Optimal learning rate found: {model.hparams.lr}')\n",
    "\n",
    "    # ------------\n",
    "    # Logger\n",
    "    # ------------\n",
    "    pl_logger = construct_pl_logger(args)  # Log everything to an external logger\n",
    "    trainer.logger = pl_logger  # Assign specified logger (e.g. TensorBoardLogger) to Trainer instance\n",
    "\n",
    "    # -----------\n",
    "    # Callbacks\n",
    "    # -----------\n",
    "    # Create and use callbacks\n",
    "    mode = 'min' if 'ce' in args.metric_to_track else 'max'\n",
    "    early_stop_callback = pl.callbacks.EarlyStopping(monitor=args.metric_to_track,\n",
    "                                                     mode=mode,\n",
    "                                                     min_delta=args.min_delta,\n",
    "                                                     patience=args.patience)\n",
    "    ckpt_callback = pl.callbacks.ModelCheckpoint(\n",
    "        monitor=args.metric_to_track,\n",
    "        mode=mode,\n",
    "        verbose=True,\n",
    "        save_last=True,\n",
    "        save_top_k=3,\n",
    "        filename=template_ckpt_filename  # Warning: May cause a race condition if calling trainer.test() with many GPUs\n",
    "    )\n",
    "    lr_monitor_callback = pl.callbacks.LearningRateMonitor(logging_interval='step', log_momentum=True)\n",
    "    trainer.callbacks = [early_stop_callback, ckpt_callback, lr_monitor_callback]\n",
    "\n",
    "    # ------------\n",
    "    # Restore\n",
    "    # ------------\n",
    "    # If using WandB, download checkpoint artifact from their servers if the checkpoint is not already stored locally\n",
    "    if use_wandb_logger and args.ckpt_name != '' and not os.path.exists(ckpt_path):\n",
    "        checkpoint_reference = f'{args.entity}/{args.project_name}/model-{args.run_id}:best'\n",
    "        artifact = trainer.logger.experiment.use_artifact(checkpoint_reference, type='model')\n",
    "        artifact_dir = artifact.download()\n",
    "        model = model.load_from_checkpoint(Path(artifact_dir) / 'model.ckpt',\n",
    "                                           use_wandb_logger=use_wandb_logger,\n",
    "                                           batch_size=args.batch_size,\n",
    "                                           lr=args.lr,\n",
    "                                           weight_decay=args.weight_decay)\n",
    "\n",
    "    # -------------\n",
    "    # Training\n",
    "    # -------------\n",
    "    # Train with the provided model and DataModule\n",
    "    trainer.fit(model=model, datamodule=db5_data_module)\n",
    "\n",
    "    # -------------\n",
    "    # Testing\n",
    "    # -------------\n",
    "    trainer.test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------\n",
    "# Jupyter\n",
    "# -----------\n",
    "sys.argv = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------\n",
    "# Arguments\n",
    "# -----------\n",
    "# Collect all arguments\n",
    "parser = collect_args()\n",
    "\n",
    "# Parse all known and unknown arguments\n",
    "args, unparsed_argv = parser.parse_known_args()\n",
    "\n",
    "# Let the model add what it wants\n",
    "parser = LitNeiA.add_model_specific_args(parser)\n",
    "\n",
    "# Re-parse all known and unknown arguments after adding those that are model specific\n",
    "args, unparsed_argv = parser.parse_known_args()\n",
    "\n",
    "# TODO: Manually set arguments within a Jupyter notebook from here\n",
    "args.model_name = \"neia\"\n",
    "args.multi_gpu_backend = \"dp\"\n",
    "args.db5_data_dir = \"../project/datasets/DB5/final/raw\"\n",
    "args.process_complexes = True\n",
    "args.batch_size = 1  # Note: `batch_size` must be `1` for compatibility with the current model implementation\n",
    "\n",
    "# Set Lightning-specific parameter values before constructing Trainer instance\n",
    "args.max_time = {'hours': args.max_hours, 'minutes': args.max_minutes}\n",
    "args.max_epochs = args.num_epochs\n",
    "args.profiler = args.profiler_method\n",
    "args.accelerator = args.multi_gpu_backend\n",
    "args.auto_select_gpus = args.auto_choose_gpus\n",
    "args.gpus = args.num_gpus\n",
    "args.num_nodes = args.num_compute_nodes\n",
    "args.precision = args.gpu_precision\n",
    "args.accumulate_grad_batches = args.accum_grad_batches\n",
    "args.gradient_clip_val = args.grad_clip_val\n",
    "args.gradient_clip_algo = args.grad_clip_algo\n",
    "args.stochastic_weight_avg = args.stc_weight_avg\n",
    "args.deterministic = True  # Make LightningModule's training reproducible\n",
    "\n",
    "# Set plugins for Lightning\n",
    "args.plugins = [\n",
    "    # 'ddp_sharded',  # For sharded model training (to reduce GPU requirements)\n",
    "    # DDPPlugin(find_unused_parameters=False),\n",
    "]\n",
    "\n",
    "# Finalize all arguments as necessary\n",
    "args = process_args(args)\n",
    "\n",
    "# Begin execution of model training with given args above\n",
    "main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DIPS-Plus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
